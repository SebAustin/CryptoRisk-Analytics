#!/usr/bin/env python3
"""
CryptoRisk Analytics - Pre-Calculate All KPIs
This script generates pre-calculated KPI tables at the correct level of detail
to eliminate complex LOD expressions and table calculations in the semantic layer.

Run this AFTER prepare_crypto_data.py to generate additional KPI tables.
"""

import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Configuration
DATA_DIR = Path(__file__).parent.parent / "data" / "raw"

def load_existing_data():
    """Load data generated by prepare_crypto_data.py"""
    print("\n=== Loading Existing Data ===")
    
    prices_df = pd.read_csv(DATA_DIR / 'crypto_prices_daily_2020_2024.csv', parse_dates=['date'])
    positions_df = pd.read_csv(DATA_DIR / 'portfolio_positions_current.csv', parse_dates=['as_of_date'])
    trades_df = pd.read_csv(DATA_DIR / 'trades_history_sample.csv', parse_dates=['trade_date'])
    crypto_ref_df = pd.read_csv(DATA_DIR / 'crypto_reference.csv')
    
    print(f"âœ“ Loaded {len(prices_df):,} price records")
    print(f"âœ“ Loaded {len(positions_df):,} position records")
    print(f"âœ“ Loaded {len(trades_df):,} trade records")
    print(f"âœ“ Loaded {len(crypto_ref_df)} cryptocurrencies")
    
    return prices_df, positions_df, trades_df, crypto_ref_df

def calculate_portfolio_volatility_timeseries(prices_df, positions_df):
    """
    Calculate rolling volatility for each portfolio for each date.
    Output: portfolio_id, date, volatility_30d, volatility_90d, volatility_365d
    """
    print("\n=== Calculating Portfolio Volatility Time Series ===")
    
    volatility_records = []
    
    # Get unique portfolios and their allocations
    portfolios = positions_df.groupby(['portfolio_id', 'portfolio_name']).agg({
        'symbol': lambda x: list(x.unique()),
        'target_weight': lambda x: list(x)
    }).reset_index()
    
    for _, portfolio in portfolios.iterrows():
        portfolio_id = portfolio['portfolio_id']
        portfolio_name = portfolio['portfolio_name']
        symbols = portfolio['symbol']
        weights_dict = dict(zip(portfolio['symbol'], portfolio['target_weight']))
        
        print(f"   Processing {portfolio_name} ({len(symbols)} assets)...")
        
        # Get returns for all assets in portfolio
        portfolio_returns = []
        for symbol in symbols:
            asset_returns = prices_df[prices_df['symbol'] == symbol][['date', 'daily_return']].copy()
            asset_returns = asset_returns.rename(columns={'daily_return': symbol})
            asset_returns = asset_returns.set_index('date')
            portfolio_returns.append(asset_returns)
        
        # Combine into single dataframe
        if portfolio_returns:
            combined_returns = pd.concat(portfolio_returns, axis=1)
            
            # Calculate weighted portfolio returns
            weighted_returns = pd.Series(0.0, index=combined_returns.index)
            for symbol in symbols:
                if symbol in combined_returns.columns:
                    weight = weights_dict.get(symbol, 0)
                    weighted_returns += combined_returns[symbol].fillna(0) * weight
            
            # Calculate rolling volatilities for each date
            for date in combined_returns.index:
                # 30-day volatility
                returns_30d = weighted_returns.loc[:date].tail(30)
                vol_30d = returns_30d.std() * np.sqrt(365) if len(returns_30d) >= 30 else None
                
                # 90-day volatility
                returns_90d = weighted_returns.loc[:date].tail(90)
                vol_90d = returns_90d.std() * np.sqrt(365) if len(returns_90d) >= 90 else None
                
                # 365-day volatility
                returns_365d = weighted_returns.loc[:date].tail(365)
                vol_365d = returns_365d.std() * np.sqrt(365) if len(returns_365d) >= 365 else None
                
                # Downside volatility (365d)
                downside_returns = returns_365d[returns_365d < 0]
                downside_vol_365d = downside_returns.std() * np.sqrt(365) if len(downside_returns) >= 30 else None
                
                volatility_records.append({
                    'portfolio_id': portfolio_id,
                    'portfolio_name': portfolio_name,
                    'date': date,
                    'volatility_30d': round(vol_30d, 6) if vol_30d is not None else None,
                    'volatility_90d': round(vol_90d, 6) if vol_90d is not None else None,
                    'volatility_365d': round(vol_365d, 6) if vol_365d is not None else None,
                    'downside_volatility_365d': round(downside_vol_365d, 6) if downside_vol_365d is not None else None
                })
    
    df = pd.DataFrame(volatility_records)
    print(f"   Generated {len(df):,} volatility records")
    
    return df

def calculate_portfolio_var_current(prices_df, positions_df):
    """
    Calculate VaR for each portfolio using LATEST positions only.
    Output: portfolio_id, as_of_date, var_95, var_99, portfolio_value
    """
    print("\n=== Calculating Current Portfolio VaR ===")
    
    var_records = []
    
    # Get latest date
    latest_date = positions_df['as_of_date'].max()
    latest_positions = positions_df[positions_df['as_of_date'] == latest_date]
    
    print(f"   Using positions as of {latest_date.date()}")
    
    for portfolio_id in latest_positions['portfolio_id'].unique():
        pf_positions = latest_positions[latest_positions['portfolio_id'] == portfolio_id]
        portfolio_name = pf_positions['portfolio_name'].iloc[0]
        portfolio_value = pf_positions['position_value'].sum()
        
        # Get historical returns for all assets
        symbols = pf_positions['symbol'].tolist()
        weights = (pf_positions['position_value'] / portfolio_value).tolist()
        
        # Calculate portfolio returns (align dates)
        portfolio_returns_df = prices_df[prices_df['symbol'].isin(symbols)][['date', 'symbol', 'daily_return']].pivot(
            index='date', columns='symbol', values='daily_return'
        )
        
        # Calculate weighted portfolio returns
        pf_returns = pd.Series(0.0, index=portfolio_returns_df.index)
        for symbol, weight in zip(symbols, weights):
            if symbol in portfolio_returns_df.columns:
                pf_returns += portfolio_returns_df[symbol].fillna(0) * weight
        
        if len(pf_returns) > 0:
            pf_returns = pf_returns.dropna()
            
            # Calculate VaR (5th and 1st percentile)
            var_95_pct = np.percentile(pf_returns.values, 5)
            var_99_pct = np.percentile(pf_returns.values, 1)
            
            # Convert to dollar amounts
            var_95 = var_95_pct * portfolio_value
            var_99 = var_99_pct * portfolio_value
            
            var_records.append({
                'portfolio_id': portfolio_id,
                'portfolio_name': portfolio_name,
                'as_of_date': latest_date,
                'portfolio_value': round(portfolio_value, 2),
                'var_95': round(var_95, 2),
                'var_99': round(var_99, 2),
                'var_95_pct': round(var_95_pct * 100, 4),
                'var_99_pct': round(var_99_pct * 100, 4)
            })
            
            print(f"   {portfolio_name}: VaR 95% = ${var_95:,.2f}, VaR 99% = ${var_99:,.2f}")
    
    df = pd.DataFrame(var_records)
    return df

def calculate_portfolio_risk_adjusted_returns(prices_df, positions_df, volatility_df):
    """
    Calculate Sharpe, Sortino, Beta, Alpha, Correlation for each portfolio.
    Uses latest positions and full historical returns.
    Output: portfolio_id, as_of_date, sharpe_ratio, sortino_ratio, beta, alpha, correlation
    """
    print("\n=== Calculating Risk-Adjusted Returns ===")
    
    metrics_records = []
    
    # Get latest date and volatilities
    latest_date = positions_df['as_of_date'].max()
    latest_positions = positions_df[positions_df['as_of_date'] == latest_date]
    latest_volatility = volatility_df[volatility_df['date'] == latest_date]
    
    # Get BTC returns for beta/alpha/correlation
    btc_returns = prices_df[prices_df['symbol'] == 'BTC'][['date', 'daily_return']].copy()
    btc_returns = btc_returns.rename(columns={'daily_return': 'btc_return'})
    
    for portfolio_id in latest_positions['portfolio_id'].unique():
        pf_positions = latest_positions[latest_positions['portfolio_id'] == portfolio_id]
        portfolio_name = pf_positions['portfolio_name'].iloc[0]
        portfolio_value = pf_positions['position_value'].sum()
        cost_basis = portfolio_value - pf_positions['unrealized_pnl'].sum()
        
        # Portfolio return (annualized)
        portfolio_return_annualized = (pf_positions['unrealized_pnl'].sum() / cost_basis) if cost_basis > 0 else 0
        
        # Get volatilities
        pf_vol = latest_volatility[latest_volatility['portfolio_id'] == portfolio_id]
        vol_365d = pf_vol['volatility_365d'].iloc[0] if len(pf_vol) > 0 and pd.notna(pf_vol['volatility_365d'].iloc[0]) else 0.25
        downside_vol_365d = pf_vol['downside_volatility_365d'].iloc[0] if len(pf_vol) > 0 and pd.notna(pf_vol['downside_volatility_365d'].iloc[0]) else 0.20
        
        # Calculate Sharpe Ratio
        risk_free_rate = 0.02  # 2% annual
        sharpe_ratio = (portfolio_return_annualized - risk_free_rate) / vol_365d if vol_365d > 0 else 0
        
        # Calculate Sortino Ratio
        sortino_ratio = (portfolio_return_annualized - risk_free_rate) / downside_vol_365d if downside_vol_365d > 0 else 0
        
        # Calculate portfolio returns series for beta/correlation
        symbols = pf_positions['symbol'].tolist()
        weights = (pf_positions['position_value'] / portfolio_value).tolist()
        
        portfolio_returns_list = []
        for symbol, weight in zip(symbols, weights):
            asset_returns = prices_df[prices_df['symbol'] == symbol][['date', 'daily_return']].copy()
            asset_returns['weighted_return'] = asset_returns['daily_return'] * weight
            portfolio_returns_list.append(asset_returns[['date', 'weighted_return']])
        
        if portfolio_returns_list:
            # Combine portfolio returns
            pf_returns_df = portfolio_returns_list[0].copy()
            for i in range(1, len(portfolio_returns_list)):
                pf_returns_df = pf_returns_df.merge(portfolio_returns_list[i], on='date', how='outer', suffixes=('', f'_{i}'))
            
            # Sum all weighted returns
            weighted_cols = [col for col in pf_returns_df.columns if 'weighted_return' in col]
            pf_returns_df['portfolio_return'] = pf_returns_df[weighted_cols].sum(axis=1)
            
            # Merge with BTC returns
            combined = pf_returns_df[['date', 'portfolio_return']].merge(btc_returns, on='date', how='inner')
            combined = combined.dropna()
            
            if len(combined) > 30:
                # Calculate Beta
                covariance = combined['portfolio_return'].cov(combined['btc_return'])
                btc_variance = combined['btc_return'].var()
                beta = covariance / btc_variance if btc_variance > 0 else 1.0
                
                # Calculate Correlation
                correlation = combined['portfolio_return'].corr(combined['btc_return'])
                
                # Calculate BTC return (annualized)
                btc_return_annualized = combined['btc_return'].mean() * 365
                
                # Calculate Alpha
                alpha = portfolio_return_annualized - (beta * btc_return_annualized)
            else:
                beta = 1.0
                correlation = 0.5
                alpha = 0.0
        else:
            beta = 1.0
            correlation = 0.5
            alpha = 0.0
        
        metrics_records.append({
            'portfolio_id': portfolio_id,
            'portfolio_name': portfolio_name,
            'as_of_date': latest_date,
            'portfolio_value': round(portfolio_value, 2),
            'cost_basis': round(cost_basis, 2),
            'portfolio_return_annualized': round(portfolio_return_annualized, 6),
            'sharpe_ratio': round(sharpe_ratio, 4),
            'sortino_ratio': round(sortino_ratio, 4),
            'beta_vs_btc': round(beta, 4),
            'alpha': round(alpha, 6),
            'correlation_to_btc': round(correlation, 4)
        })
        
        print(f"   {portfolio_name}: Sharpe={sharpe_ratio:.2f}, Beta={beta:.2f}, Alpha={alpha*100:.2f}%")
    
    df = pd.DataFrame(metrics_records)
    return df

def calculate_portfolio_concentration_metrics(positions_df):
    """
    Calculate position-level metrics and portfolio concentration.
    Output: portfolio_id, as_of_date, symbol, position_weight, hhi, liquid_assets_pct
    """
    print("\n=== Calculating Portfolio Concentration Metrics ===")
    
    concentration_records = []
    
    # Get latest date
    latest_date = positions_df['as_of_date'].max()
    latest_positions = positions_df[positions_df['as_of_date'] == latest_date]
    
    for portfolio_id in latest_positions['portfolio_id'].unique():
        pf_positions = latest_positions[latest_positions['portfolio_id'] == portfolio_id]
        portfolio_name = pf_positions['portfolio_name'].iloc[0]
        total_value = pf_positions['position_value'].sum()
        
        # Calculate position weights
        pf_positions['position_weight'] = pf_positions['position_value'] / total_value
        
        # Calculate HHI (Herfindahl Index)
        hhi = (pf_positions['position_weight'] ** 2).sum()
        
        # Identify liquid assets (Layer 1 and Exchange Token categories)
        # We'll need to merge with crypto reference
        # For now, assuming BTC, ETH, BNB are liquid
        liquid_symbols = ['BTC', 'ETH', 'BNB', 'SOL', 'ADA', 'XRP', 'DOT']
        liquid_value = pf_positions[pf_positions['symbol'].isin(liquid_symbols)]['position_value'].sum()
        liquid_assets_pct = (liquid_value / total_value) * 100 if total_value > 0 else 0
        
        # Per-position records
        for _, position in pf_positions.iterrows():
            concentration_records.append({
                'portfolio_id': portfolio_id,
                'portfolio_name': portfolio_name,
                'as_of_date': latest_date,
                'symbol': position['symbol'],
                'position_value': round(position['position_value'], 2),
                'position_weight': round(position['position_weight'], 6),
                'target_weight': position['target_weight'],
                'weight_drift': round(position['position_weight'] - position['target_weight'], 6),
                'is_overweight': position['position_weight'] > position['target_weight'] + 0.10,
                'is_underweight': position['position_weight'] < position['target_weight'] - 0.10,
                'portfolio_hhi': round(hhi, 6),
                'portfolio_liquid_assets_pct': round(liquid_assets_pct, 2)
            })
        
        print(f"   {portfolio_name}: HHI={hhi:.4f}, Liquid Assets={liquid_assets_pct:.1f}%")
    
    df = pd.DataFrame(concentration_records)
    return df

def calculate_portfolio_24h_change(positions_df):
    """
    Calculate 24h change for each portfolio.
    Output: portfolio_id, date, portfolio_value, change_24h, change_24h_pct
    """
    print("\n=== Calculating Portfolio 24h Change ===")
    
    change_records = []
    
    # Group by portfolio and date
    daily_values = positions_df.groupby(['portfolio_id', 'portfolio_name', 'as_of_date'])['position_value'].sum().reset_index()
    daily_values = daily_values.sort_values(['portfolio_id', 'as_of_date'])
    
    for portfolio_id in daily_values['portfolio_id'].unique():
        pf_data = daily_values[daily_values['portfolio_id'] == portfolio_id].copy()
        portfolio_name = pf_data['portfolio_name'].iloc[0]
        
        # Calculate 24h change
        pf_data['prev_value'] = pf_data['position_value'].shift(1)
        pf_data['change_24h'] = pf_data['position_value'] - pf_data['prev_value']
        pf_data['change_24h_pct'] = (pf_data['change_24h'] / pf_data['prev_value']) * 100
        
        for _, row in pf_data.iterrows():
            if pd.notna(row['change_24h']):
                change_records.append({
                    'portfolio_id': portfolio_id,
                    'portfolio_name': portfolio_name,
                    'date': row['as_of_date'],
                    'portfolio_value': round(row['position_value'], 2),
                    'change_24h': round(row['change_24h'], 2),
                    'change_24h_pct': round(row['change_24h_pct'], 4)
                })
    
    df = pd.DataFrame(change_records)
    print(f"   Generated {len(df):,} daily change records")
    
    return df

def save_kpi_tables(volatility_df, var_df, risk_adj_df, concentration_df, change_24h_df):
    """Save all pre-calculated KPI tables to CSV."""
    print("\n=== Saving Pre-Calculated KPI Tables ===")
    
    # 1. Portfolio Volatility Time Series
    volatility_df['date'] = pd.to_datetime(volatility_df['date']).dt.strftime('%Y-%m-%d')
    output_path = DATA_DIR / 'kpi_portfolio_volatility_timeseries.csv'
    volatility_df.to_csv(output_path, index=False)
    file_size_mb = output_path.stat().st_size / (1024 * 1024)
    print(f"âœ“ Created {output_path.name} ({len(volatility_df):,} records, {file_size_mb:.2f} MB)")
    
    # 2. Current Portfolio VaR
    var_df['as_of_date'] = pd.to_datetime(var_df['as_of_date']).dt.strftime('%Y-%m-%d')
    output_path = DATA_DIR / 'kpi_portfolio_var_current.csv'
    var_df.to_csv(output_path, index=False)
    file_size_mb = output_path.stat().st_size / (1024 * 1024)
    print(f"âœ“ Created {output_path.name} ({len(var_df):,} records, {file_size_mb:.2f} MB)")
    
    # 3. Risk-Adjusted Returns
    risk_adj_df['as_of_date'] = pd.to_datetime(risk_adj_df['as_of_date']).dt.strftime('%Y-%m-%d')
    output_path = DATA_DIR / 'kpi_portfolio_risk_adjusted_returns.csv'
    risk_adj_df.to_csv(output_path, index=False)
    file_size_mb = output_path.stat().st_size / (1024 * 1024)
    print(f"âœ“ Created {output_path.name} ({len(risk_adj_df):,} records, {file_size_mb:.2f} MB)")
    
    # 4. Portfolio Concentration
    concentration_df['as_of_date'] = pd.to_datetime(concentration_df['as_of_date']).dt.strftime('%Y-%m-%d')
    output_path = DATA_DIR / 'kpi_portfolio_concentration.csv'
    concentration_df.to_csv(output_path, index=False)
    file_size_mb = output_path.stat().st_size / (1024 * 1024)
    print(f"âœ“ Created {output_path.name} ({len(concentration_df):,} records, {file_size_mb:.2f} MB)")
    
    # 5. Portfolio 24h Change
    change_24h_df['date'] = pd.to_datetime(change_24h_df['date']).dt.strftime('%Y-%m-%d')
    output_path = DATA_DIR / 'kpi_portfolio_24h_change.csv'
    change_24h_df.to_csv(output_path, index=False)
    file_size_mb = output_path.stat().st_size / (1024 * 1024)
    print(f"âœ“ Created {output_path.name} ({len(change_24h_df):,} records, {file_size_mb:.2f} MB)")
    
    # Calculate total KPI table size
    total_size = sum([
        (DATA_DIR / f).stat().st_size for f in [
            'kpi_portfolio_volatility_timeseries.csv',
            'kpi_portfolio_var_current.csv',
            'kpi_portfolio_risk_adjusted_returns.csv',
            'kpi_portfolio_concentration.csv',
            'kpi_portfolio_24h_change.csv'
        ]
    ]) / (1024 * 1024)
    
    print(f"\nðŸ“¦ Total KPI tables size: {total_size:.2f} MB")
    
    return total_size

def main():
    """Main execution function."""
    print("\n" + "="*70)
    print("CryptoRisk Analytics - Pre-Calculate All KPIs")
    print("="*70)
    print(f"Output directory: {DATA_DIR}\n")
    
    # Load existing data
    prices_df, positions_df, trades_df, crypto_ref_df = load_existing_data()
    
    # Calculate all KPIs
    print("\n" + "="*70)
    print("CALCULATING PRE-COMPUTED KPI TABLES")
    print("="*70)
    
    # 1. Portfolio Volatility Time Series (rolling windows)
    volatility_df = calculate_portfolio_volatility_timeseries(prices_df, positions_df)
    
    # 2. Current Portfolio VaR
    var_df = calculate_portfolio_var_current(prices_df, positions_df)
    
    # 3. Risk-Adjusted Returns (Sharpe, Sortino, Beta, Alpha)
    risk_adj_df = calculate_portfolio_risk_adjusted_returns(prices_df, positions_df, volatility_df)
    
    # 4. Portfolio Concentration Metrics
    concentration_df = calculate_portfolio_concentration_metrics(positions_df)
    
    # 5. Portfolio 24h Change
    change_24h_df = calculate_portfolio_24h_change(positions_df)
    
    # Save all KPI tables
    total_kpi_size = save_kpi_tables(volatility_df, var_df, risk_adj_df, concentration_df, change_24h_df)
    
    # Summary
    print("\n" + "="*70)
    print("âœ… PRE-CALCULATED KPI TABLES COMPLETE!")
    print("="*70)
    print("\nðŸ“Š Generated Tables:")
    print(f"  1. kpi_portfolio_volatility_timeseries.csv - {len(volatility_df):,} records")
    print(f"  2. kpi_portfolio_var_current.csv - {len(var_df):,} records")
    print(f"  3. kpi_portfolio_risk_adjusted_returns.csv - {len(risk_adj_df):,} records")
    print(f"  4. kpi_portfolio_concentration.csv - {len(concentration_df):,} records")
    print(f"  5. kpi_portfolio_24h_change.csv - {len(change_24h_df):,} records")
    print(f"\nðŸ“¦ Total size: {total_kpi_size:.2f} MB")
    
    print("\nâœ… Next Steps:")
    print("1. Upload these 5 new CSV files to Tableau Data Cloud")
    print("2. Create DMOs for each KPI table")
    print("3. Update semantic layer to reference pre-calculated fields")
    print("4. Enjoy simple SUM/AVG aggregations instead of complex LOD!")
    print("\n" + "="*70)

if __name__ == "__main__":
    main()
